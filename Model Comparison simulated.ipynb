{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "lucky-sleeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import Tensor\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.distributions import Distribution\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from arff2pandas import a2p\n",
    "import dill\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "encouraging-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "from VAE.models import BaseVAEprob, BaseLSTM_VAEprob, Neural_Stat\n",
    "from generators.generate_data_tfh import SERSGenerator\n",
    "from utils import plot_sequences\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-scholarship",
   "metadata": {},
   "source": [
    "### Load NS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "unsigned-bunny",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_NS(nn.Module):\n",
    "    def __init__(self, seq_len, n_features, hidden_dim=50, num_layers=1, bidirectional=True,\n",
    "                 latent_z:int=1, z_layers:int=1, latent_c:int=1):\n",
    "        super(Encoder_NS, self).__init__()\n",
    "\n",
    "        self.seq_len, self.n_features = seq_len, n_features\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.n_features,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=self.bidirectional\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.reshape((-1, self.seq_len, self.n_features))\n",
    "\n",
    "        #(batch, hidden)\n",
    "        x, (hidden_n, _) = self.lstm(x)\n",
    "\n",
    "        if self.bidirectional:\n",
    "            hidden_n = torch.cat([hidden_n[-1,:, :], hidden_n[-2,:,:]], dim=1).unsqueeze(0)\n",
    "        else:\n",
    "            hidden_n = hidden_n[-1, :, :]\n",
    "\n",
    "        hidden_n = hidden_n.reshape(batch_size, -1)\n",
    "        #print(hidden_n.shape)\n",
    "        return hidden_n\n",
    "class Decoder_NS(nn.Module):\n",
    "    def __init__(self, latent_z:int, z_layers:int, latent_c:int,\n",
    "                 seq_len, n_features=1, hidden_dim=50, num_layers=1, bidirectional=True):\n",
    "        super(Decoder_NS, self).__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.hidden_dim, self.n_features = hidden_dim, n_features\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.latent_z = latent_z\n",
    "        self.z_layers = z_layers\n",
    "        self.latent_c = latent_c\n",
    "        \n",
    "        self.lstm_depth = self.num_layers + self.num_layers * int(self.bidirectional)\n",
    "        \n",
    "        self.latent_to_hidden = nn.Linear(self.latent_c+self.latent_z*self.z_layers,\n",
    "                                          self.hidden_dim)\n",
    "        \n",
    "        self.hidden_to_mu = nn.Linear(self.hidden_dim + int(self.bidirectional) * self.hidden_dim, self.n_features)\n",
    "        self.hidden_to_sig = nn.Linear(self.hidden_dim + int(self.bidirectional) * self.hidden_dim, self.n_features)\n",
    "        \n",
    "            \n",
    "            \n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.decoder_inputs = torch.zeros(self.seq_len, self.n_features, requires_grad=True).type(torch.cuda.FloatTensor)\n",
    "            self.c_0 = torch.zeros(self.lstm_depth, self.hidden_dim, requires_grad=True).type(torch.cuda.FloatTensor)\n",
    "        else:\n",
    "            self.decoder_inputs = torch.zeros(self.seq_len, self.n_features, requires_grad=True).type(torch.FloatTensor)\n",
    "            self.c_0 = torch.zeros(self.lstm_depth, self.hidden_dim, requires_grad=True).type(torch.FloatTensor)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.n_features,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=self.bidirectional\n",
    "        )\n",
    "        \n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # https://github.com/hellojinwoo/TorchCoder/blob/master/autoencoders/rae.py\n",
    "        # THIS LAYER CAN BE EXCLUDED\n",
    "        batch_size = x.size(0)\n",
    "        h_state = self.latent_to_hidden(x)        \n",
    "        \n",
    "        h_0 = torch.stack([h_state for _ in range(self.lstm_depth)])\n",
    "        #print(f\"h_0: {h_0.shape}\")\n",
    "        inputs = torch.stack([self.decoder_inputs for _ in range(batch_size)])\n",
    "        #print(f\"inputs: {inputs.shape}\")\n",
    "        c_inp = torch.stack([self.c_0 for _ in range(batch_size)]).permute(1,0,2).contiguous()\n",
    "        #print(f\"c_inp: {c_inp.shape}\")\n",
    "        \n",
    "        decoder_output, _ = self.lstm(inputs, (h_0, c_inp))\n",
    "        #print(f\"DECOER OUTPUT: {decoder_output.shape}\")        \n",
    "        \n",
    "        out_mu = self.hidden_to_mu(decoder_output).squeeze(-1)\n",
    "        out_sig = self.hidden_to_sig(decoder_output).squeeze(-1)\n",
    "        \n",
    "        out = torch.cat([out_mu, out_sig], axis=1)\n",
    "        #print(out.shape)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "living-inspection",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('NS_lstm_init_cpu.pkl', 'rb') as f:\n",
    "    state_dict_loaded = torch.load(f,map_location=torch.device('cpu'))\n",
    "\n",
    "encoder_NS = Encoder_NS(**state_dict_loaded[\"encoder_decoder_params\"]).to(device)\n",
    "decoder_NS = Decoder_NS(**state_dict_loaded[\"encoder_decoder_params\"]).to(device)\n",
    "model_NS = Neural_Stat(Encoder=encoder_NS, Decoder=decoder_NS, **state_dict_loaded[\"model_params\"]).to(device)\n",
    "model_NS.load_state_dict(state_dict_loaded[\"model\"]())\n",
    "# Optimizer\n",
    "optimizer_NS = torch.optim.Adam(model_NS.parameters(), lr=state_dict_loaded[\"LEARNING_RATE\"])\n",
    "optimizer_NS.load_state_dict(state_dict_loaded[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-longer",
   "metadata": {},
   "source": [
    "### Load base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_base = Encoder(**state_dict_loaded[\"encoder_decoder_params\"]).to(device)\n",
    "\n",
    "decoder_base = Decoder(**state_dict_loaded[\"encoder_decoder_params\"]).to(device)\n",
    "\n",
    "model_base = BaseLSTM_VAEprob(encoder=encoder_new, decoder=decoder_new, **state_dict_loaded[\"model_params\"]).to(device)\n",
    "\n",
    "model_base.load_state_dict(state_dict_loaded[\"model\"]())\n",
    "\n",
    "optimizer_base = torch.optim.Adam(model_new.parameters(), lr=state_dict_loaded[\"LEARNING_RATE\"])\n",
    "\n",
    "optimizer_base.load_state_dict(state_dict_loaded[\"optimizer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
